{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Machine learning HW 3\n",
    "Spring 2019\n",
    "pete rodrigue\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from IPython.core.pylabtools import figsize\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\edwar.WJM-SONYLAPTOP\\\\Documents\\\\GitHub\\\\ML_spring_2019\\\\exercise three'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your working directory to the root folder of the repository (ML_spring_2019) on your computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You may need to change the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\edwar.WJM-SONYLAPTOP\\\\Documents\\\\GitHub\\\\ML_spring_2019\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define all our functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_peek_at_data(path, summary=False):\n",
    "    '''\n",
    "    Loads our data and returns a pandas dataframe.\n",
    "    This function also saves a csv file with descriptive statistics for all\n",
    "    our variables to our figures folder.\n",
    "    '''\n",
    "    separator = '************************\\n************************\\n\\n'\n",
    "    df = pd.read_csv(path)\n",
    "    print(separator)\n",
    "    print('Head of data:')\n",
    "    print(df.head(5))\n",
    "    print(separator)\n",
    "    print('Tail of data:')\n",
    "    print(df.tail(5))\n",
    "    print(separator)\n",
    "    print('column names of data:')\n",
    "    print(df.columns)\n",
    "    print(separator)\n",
    "    print('number of rows of data:')\n",
    "    print(len(df))\n",
    "    print(separator)\n",
    "\n",
    "    if summary:\n",
    "        print(\"\\n\\n\\nSummary of data:\")\n",
    "        print(df.describe())\n",
    "        df.describe().to_csv('figures/summary.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_graphs(df, normal_qq_plots=False):\n",
    "    '''\n",
    "    Takes our dataframe, fills in missing values with the median,\n",
    "    and outputs a series of plots:\n",
    "            - Normal qq plots for each variable\n",
    "            - Boxplots for each variable\n",
    "            - Histograms for each variable\n",
    "        - A correlation plot for all our variables\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "    '''\n",
    "    df_temp = df._get_numeric_data()\n",
    "    fill_missing(df_temp)\n",
    "    g = sns.heatmap(df[df.columns.difference(\n",
    "                 ['PersonID',\n",
    "                  'SeriousDlqin2yrs',\n",
    "                  'zipcode',\n",
    "                  'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "                  'NumberOfTimes90DaysLate'])].corr())\n",
    "    plt.savefig('figures/correlation_plot')\n",
    "    plt.close()\n",
    "    for col in df_temp.columns:\n",
    "        plt.clf()\n",
    "        mycol = df_temp[col][df_temp[col].notna()]\n",
    "        print('skew', ' for col ', mycol.name, 'is:', mycol.skew())\n",
    "        if abs(mycol.skew()) > 10:\n",
    "            path = \"figures/\" + col + \"log_transformed\"\n",
    "            g = sns.distplot(mycol)\n",
    "            g.set_title(col + \" dist, log_transformed\")\n",
    "            g.set(xscale='log')\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "            if normal_qq_plots:\n",
    "                path = \"figures/\" + col + \\\n",
    "                       \" normal_qq_plot log trans\"\n",
    "                g = stats.probplot(np.log(df[col]+.0001),\n",
    "                                   dist=\"norm\", plot=pylab)\n",
    "                plt.title(col + \" normal_qq log transformed\")\n",
    "                plt.savefig(path)\n",
    "        else:\n",
    "            path = \"figures/\" + col\n",
    "            g = sns.distplot(mycol)\n",
    "            g.set_title(col + \" distribution\")\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "            if normal_qq_plots:\n",
    "                path = \"figures/\" + col + \" normal_qq_plot\"\n",
    "                g = stats.probplot(df[col], dist=\"norm\", plot=pylab)\n",
    "                plt.title(col + \" normal_qq\")\n",
    "                plt.savefig(path)\n",
    "        plt.clf()\n",
    "        path = \"figures/\" + col + \" boxplot\"\n",
    "        g = sns.boxplot(mycol)\n",
    "        plt.savefig(path)\n",
    "\n",
    "\n",
    "def fill_missing(df):\n",
    "    '''\n",
    "    Fill missing numerica data in our data frame with the median value of that\n",
    "    variable. Modifies the dataframe in place. Does not return anything.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "    '''\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            median_val = df[col].median()\n",
    "            df[col].fillna(median_val, inplace=True)\n",
    "\n",
    "\n",
    "def descretize_var(df, var, num_groups):\n",
    "    '''\n",
    "    Takes one of our variables and splits it into discrete groups.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        var (str): the column in our dataframe that we want to make a\n",
    "                   categorical variable from\n",
    "        num_groups (int): the number of groups our discrete variable will have\n",
    "\n",
    "    Returns: a modified dataframe.\n",
    "    '''\n",
    "    labs = list(range(1, num_groups + 1))\n",
    "    labs = [str(x) for x in labs]\n",
    "    new_var = var + '_discrete'\n",
    "    df[new_var] = pd.qcut(df[var], num_groups, labels=labs)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_dummies(df, var):\n",
    "    '''\n",
    "    Takes our dataframe and turns a specified variable into a series of\n",
    "    dummy columns. This function returns the modified dataframe.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        var (str): the column in our dataframe that we want to make dummies of\n",
    "\n",
    "    Returns: a modified dataframe.\n",
    "    '''\n",
    "    new_var_prefix = \"D_\" + var\n",
    "\n",
    "    return pd.concat([df, pd.get_dummies(df[var], prefix=new_var_prefix)],\n",
    "                     axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree_model(x_data, y_data, x_test=None,\n",
    "                   y_test=None, max_depth=5, outcome_labels=None, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our data and computes a decision tree model.\n",
    "    It saves a .dot file you can open in graphviz to see the tree.\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        max_depth (int): the maximum depth of the tree.\n",
    "        outcome_labels (list of str): the labels for our predictor variables.\n",
    "    '''\n",
    "    mymodel = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "    mymodel.fit(X=x_data, y=y_data)\n",
    "    \n",
    "    print(\"***************Tree model\")\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        print('mean of pred class is')\n",
    "        print(predicted_probs['predicted_class'].mean())\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logit_model(x_data, y_data, x_test=None, y_test=None, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our x and y data and a threshold,\n",
    "    and computes a logistic model. It exports a confusion matrix table.\n",
    "\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        threshold (float): the threshold, between 0 and 1, that we'll use to\n",
    "                           to decide if a given row is predicted to be a\n",
    "                           positive in the target class or not.\n",
    "    '''\n",
    "    mymodel = LogisticRegression()\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('***********Logistic regression')\n",
    "    print('Training set performance:')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_model(x_data, y_data, x_test=None, y_test=None, num_n=2, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our x and y data and a threshold,\n",
    "    and computes a knn model. It exports a confusion matrix table.\n",
    "\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        threshold (float): probability threshold needed to call prediction positive\n",
    "        num_n (int): the number of neighbors\n",
    "    '''\n",
    "    mymodel = KNeighborsClassifier(n_neighbors=num_n)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('************KNN')\n",
    "    print('Training set performance:')\n",
    "    print('confusion matrix')\n",
    "    print('|T neg, F pos|\\n|F neg, T pos|')\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "def run_svm_model(x_data_scaled, y_data, x_test=None,\n",
    "                  y_test=None, kernel='linear', threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    Runs and SVM model on your data\n",
    "    Note: this will run much faster if you scale your x data first\n",
    "    '''\n",
    "    mymodel = svm.SVC(kernel='linear', probability=True)\n",
    "    mymodel.fit(x_data_scaled,\n",
    "                    y_data)  \n",
    "    mypreds = mymodel.predict(x_data_scaled)    \n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data_scaled))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def run_forest(x_data, y_data, x_test=None, y_test=None,\n",
    "               my_n_estimators=10, my_max_depth=5, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    Runs a random forest model\n",
    "    Inputs:\n",
    "        x_data (pd dataframe) : our predictor data\n",
    "        y_data (pd dataframe) : our outcome data\n",
    "        n_estimators (int): the number of trees in our forest\n",
    "        max_depth (int): the max number of levels in our trees\n",
    "    '''\n",
    "    mymodel = RandomForestClassifier(n_estimators=my_n_estimators, max_depth=my_max_depth)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "def run_boosted_model(x_data, y_data, x_test=None, y_test=None,\n",
    "                      my_max_depth=5, my_n_estimators=10, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    Run a boosted decision tree model\n",
    "    '''\n",
    "    mymodel = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=5),\n",
    "                             algorithm=\"SAMME\",\n",
    "                             n_estimators=10)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "\n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "def run_bagging_model(x_data, y_data, x_test=None, y_test=None, threshold=.5, use_test_sets=False):\n",
    "    '''\n",
    "    Runs a bagging model\n",
    "    '''\n",
    "    mymodel = BaggingClassifier(KNeighborsClassifier(n_neighbors=16),\n",
    "                                 max_samples=100, max_features=1000)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        predicted_probs.loc[predicted_probs[1] >= threshold,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through models to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(x_data=None, x_data_scaled=None, y_data=None,\n",
    "                   x_test=None, x_test_scaled=None, y_test=None, use_test_data=False,\n",
    "                   run_bagging=False, run_boosted=False, run_a_forest=False,\n",
    "                   run_svm=False, run_knn=False, run_logit=False, run_tree=False,\n",
    "                   mythresholds=[.5],\n",
    "                   my_max_depth=5, my_n_estimators=10,\n",
    "                   num_n=8,\n",
    "                   outcome_labels=None,\n",
    "                  mykernel='linear'):    \n",
    "    '''\n",
    "    Compare all our models\n",
    "    '''\n",
    "    model = []\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    precision = []\n",
    "    current_threshold = []\n",
    "    if run_bagging:\n",
    "        for t in mythresholds:\n",
    "            cm = run_bagging_model(x_data, y_data, threshold=t, x_test=x_test,\n",
    "                                   y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('bagging')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_boosted:\n",
    "        for t in mythresholds:\n",
    "            cm = run_boosted_model(x_data=x_data, y_data=y_data,\n",
    "                              my_max_depth=my_max_depth, my_n_estimators=my_n_estimators, threshold=t,\n",
    "                                  x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('boosted')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_a_forest:\n",
    "        for t in mythresholds:\n",
    "            cm = run_forest(x_data, y_data, x_test, y_test,\n",
    "                        my_n_estimators, my_max_depth, t,\n",
    "                                use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('forest')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_svm:\n",
    "        for t in mythresholds:\n",
    "#            def run_svm_model(x_data_scaled, y_data, x_test=None,\n",
    "#                   y_test=None, kernel='linear', threshold=.5, use_test_sets=False):\n",
    "            cm = run_svm_model(x_data_scaled, y_data, threshold=t,\n",
    "                              x_test=x_test_scaled, y_test=y_test, use_test_sets=use_test_data, kernel=mykernel)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('svm')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_knn:\n",
    "        for t in mythresholds:\n",
    "            cm = run_knn_model(x_data, y_data, num_n=num_n, threshold=t,\n",
    "                              x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('knn')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_logit:\n",
    "        for t in mythresholds:\n",
    "            cm = run_logit_model(x_data, y_data=y_data, threshold=t,\n",
    "                                x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('logit')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_tree:\n",
    "        for t in mythresholds:\n",
    "            cm = run_tree_model(x_data,\n",
    "                       y_data=y_data,\n",
    "                       max_depth=my_max_depth,\n",
    "                       outcome_labels=outcome_labels, threshold=t,\n",
    "                               x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('tree')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    \n",
    "    rows_to_add = len(current_threshold)\n",
    "    to_plot = pd.DataFrame({'model': model + ['baseline'] * rows_to_add, \n",
    "                            'tpr': tpr + fpr[0:rows_to_add],\n",
    "                            'fpr': fpr + fpr[0:rows_to_add],\n",
    "                            'precision': precision + [None] * rows_to_add,\n",
    "                            'threshold': current_threshold + [None] * rows_to_add,\n",
    "                           })\n",
    "    print('\\n\\n')\n",
    "    for m in to_plot['model'].unique():\n",
    "        auc = -1*np.trapz(y=to_plot.loc[to_plot['model'] == m, 'tpr'],\n",
    "                      x=to_plot.loc[to_plot['model'] == m, 'fpr'])\n",
    "        print('AUC for ', m, ' is', auc)\n",
    "        \n",
    "    print('\\n\\nTable of results:')\n",
    "    print(to_plot)\n",
    "    random_number = random.randint(1,100000)\n",
    "    print(random_number)\n",
    "    to_plot.to_csv(str(random_number) + \".csv\")\n",
    "    plt.clf()\n",
    "    sns.lineplot(x=to_plot['fpr'],\n",
    "                 y=to_plot['tpr'],\n",
    "                 hue=to_plot['model'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to split data into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_using_date(data, train_start_date, train_end_date):\n",
    "    '''\n",
    "    Splits our data into test and training sets using a date the user provides\n",
    "    '''\n",
    "    train = projects.loc[projects['date_posted'].between(\n",
    "                                                    train_start_date,\n",
    "                                                    train_end_date,\n",
    "                                                    inclusive=True), :]\n",
    "    test = projects.loc[projects['date_posted'].between(\n",
    "                                                    train_end_date,\n",
    "                                                    '2014-01-01',\n",
    "                                                    inclusive=False), :]\n",
    "    \n",
    "    return [train, test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
