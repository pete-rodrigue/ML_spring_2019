{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning pipeline\n",
    "\n",
    "Machine learning,\n",
    "Spring 2019,\n",
    "pete rodrigue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pylab\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from IPython.core.pylabtools import figsize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define pipeline functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_peek_at_data(path, summary=False):\n",
    "    '''\n",
    "    Loads our data and returns a pandas dataframe.\n",
    "    This function also saves a csv file with descriptive statistics for all\n",
    "    our variables to our figures folder.\n",
    "    Returns the pandas dataframe created from the csv file\n",
    "    '''\n",
    "    separator = '************************\\n************************\\n\\n'\n",
    "    df = pd.read_csv(path)\n",
    "    print(separator)\n",
    "    print('Head of data:')\n",
    "    print(df.head(5))\n",
    "    print(separator)\n",
    "    print('Tail of data:')\n",
    "    print(df.tail(5))\n",
    "    print(separator)\n",
    "    print('column names of data:')\n",
    "    print(df.columns)\n",
    "    print(separator)\n",
    "    print('number of rows of data:')\n",
    "    print(len(df))\n",
    "    print(separator)\n",
    "\n",
    "    if summary:\n",
    "        print(\"\\n\\n\\nSummary of data:\")\n",
    "        print(df.describe())\n",
    "        # this exports a set of summary statistics to a csv file in the figures folder\n",
    "        df.describe().to_csv('figures/summary.csv')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_graphs(df, normal_qq_plots=False):\n",
    "    '''\n",
    "    Takes our dataframe, fills in missing values with the median,\n",
    "    and outputs a series of plots:\n",
    "            - Normal qq plots for each variable\n",
    "            - Boxplots for each variable\n",
    "            - Histograms for each variable\n",
    "        - A correlation plot for all our variables\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        normal_qq_plots (bool): whether you want normal qq plots\n",
    "    '''\n",
    "    # create a temporary dataframe that only has our numeric variables\n",
    "    df_temp = df._get_numeric_data()\n",
    "    # fill the missing values for now, just for ease of plotting.\n",
    "    # be aware that this is filling the missing observations using values from the entire dataset.\n",
    "    # the distributions used to fill the missing values may look different after you conduct\n",
    "    # temporal splits.\n",
    "    fill_missing(df_temp)\n",
    "    # correlation plot:\n",
    "    g = sns.heatmap(df[df.columns.difference(\n",
    "                 ['PersonID',\n",
    "                  'SeriousDlqin2yrs',\n",
    "                  'zipcode',\n",
    "                  'NumberOfTime60-89DaysPastDueNotWorse',\n",
    "                  'NumberOfTimes90DaysLate'])].corr())\n",
    "    plt.savefig('figures/correlation_plot')  # export to figures folder\n",
    "    plt.close()\n",
    "    \n",
    "    # this for loop will loop through each of our varaibles and export distribution plots to the \n",
    "    # figures folder\n",
    "    for col in df_temp.columns:\n",
    "        plt.clf()\n",
    "        mycol = df_temp[col][df_temp[col].notna()]\n",
    "        print('skew', ' for col ', mycol.name, 'is:', mycol.skew())\n",
    "        # if the skew of our distribution is large (greater than 10), log transform it\n",
    "        if abs(mycol.skew()) > 10:\n",
    "            path = \"figures/\" + col + \"log_transformed\"\n",
    "            g = sns.distplot(mycol)\n",
    "            g.set_title(col + \" dist, log_transformed\")\n",
    "            g.set(xscale='log')\n",
    "            # export our distribution plot to the figures folder\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "            if normal_qq_plots:\n",
    "                path = \"figures/\" + col + \\\n",
    "                       \" normal_qq_plot log trans\"\n",
    "                g = stats.probplot(np.log(df[col]+.0001),\n",
    "                                   dist=\"norm\", plot=pylab)\n",
    "                plt.title(col + \" normal_qq log transformed\")\n",
    "                plt.savefig(path)\n",
    "        else:\n",
    "            # if the skew is not greater than 10, just plot the raw distribution:\n",
    "            path = \"figures/\" + col\n",
    "            g = sns.distplot(mycol)\n",
    "            g.set_title(col + \" distribution\")\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "            if normal_qq_plots:\n",
    "                path = \"figures/\" + col + \" normal_qq_plot\"\n",
    "                g = stats.probplot(df[col], dist=\"norm\", plot=pylab)\n",
    "                plt.title(col + \" normal_qq\")\n",
    "                plt.savefig(path)\n",
    "        \n",
    "        # plot and export boxplots for all of our variables\n",
    "        plt.clf()\n",
    "        path = \"figures/\" + col + \" boxplot\"\n",
    "        g = sns.boxplot(mycol)\n",
    "        plt.savefig(path)\n",
    "\n",
    "\n",
    "def fill_missing(df, imputation_method='mean'):\n",
    "    '''\n",
    "    Fill missing numerica data in our data frame with the median value of that\n",
    "    variable. Modifies the dataframe in place. Does not return anything.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        imputation_method (string): the imputation method to use. \n",
    "                                    Your choices are 'median' and 'mean'.\n",
    "                                    The default is mean.\n",
    "    '''\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().any():\n",
    "            if imputation_method == 'mean':\n",
    "                imputed_val = df[col].mean()\n",
    "            else:\n",
    "                imputed_val = df[col].median()\n",
    "            df[col].fillna(imputed_val, inplace=True)\n",
    "\n",
    "\n",
    "def descretize_var(df, var, num_groups):\n",
    "    '''\n",
    "    Takes one of our variables and splits it into discrete groups.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        var (str): the column in our dataframe that we want to make a\n",
    "                   categorical variable from\n",
    "        num_groups (int): the number of groups our discrete variable will have\n",
    "\n",
    "    Returns: a modified dataframe.\n",
    "    '''\n",
    "    labs = list(range(1, num_groups + 1)) #  Get the number of groups\n",
    "    labs = [str(x) for x in labs]         #  Get the labels of the groups\n",
    "    new_var = var + '_discrete'           #create a new variable that will be discrete\n",
    "    # populate that new variable\n",
    "    df[new_var] = pd.qcut(df[var], num_groups, labels=labs)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_dummies(df, var):\n",
    "    '''\n",
    "    Takes our dataframe and turns a specified variable into a series of\n",
    "    dummy columns. This function returns the modified dataframe.\n",
    "\n",
    "    Inputs:\n",
    "        df (pandas dataframe): our dataframe we want to modify\n",
    "        var (str): the column in our dataframe that we want to make dummies of\n",
    "\n",
    "    Returns: a modified dataframe. The dummy variables will have the prefix \"D_\"\n",
    "    '''\n",
    "    new_var_prefix = \"D_\" + var\n",
    "\n",
    "    return pd.concat([df, pd.get_dummies(df[var], prefix=new_var_prefix)],\n",
    "                     axis=1)\n",
    "\n",
    "\n",
    "def print_confusion_matrix(cm):\n",
    "    '''prints a confusion matrix'''\n",
    "        print('confusion matrix')\n",
    "        print('|T neg, F pos|\\n|F neg, T pos|')\n",
    "        print(cm)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model functions. These will be combined below into one function, called \"compare_models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tree_model(x_data, y_data, x_test=None,\n",
    "                   y_test=None, max_depth=5,\n",
    "                   outcome_labels=None, threshold=50,\n",
    "                   use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our data and computes a decision tree model.\n",
    "    It saves a .dot file you can open in graphviz to see the tree.\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        max_depth (int): the maximum depth of the tree.\n",
    "        outcome_labels (list of str): the labels for our predictor variables.\n",
    "    '''\n",
    "    mymodel = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "    mymodel.fit(X=x_data, y=y_data)\n",
    "    \n",
    "    print(\"***************Tree model\")\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logit_model(x_data, y_data, x_test=None, y_test=None,\n",
    "                    threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our x and y data and a threshold,\n",
    "    and computes a logistic model. It exports a confusion matrix table.\n",
    "\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        threshold (float): the threshold, between 0 and 1, that we'll use to\n",
    "                           to decide if a given row is predicted to be a\n",
    "                           positive in the target class or not.\n",
    "    '''\n",
    "    mymodel = LogisticRegression()\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('***********Logistic regression')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_knn_model(x_data, y_data, x_test=None, y_test=None,\n",
    "                  num_n=2, threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    This function takes our x and y data and a threshold,\n",
    "    and computes a knn model. It exports a confusion matrix table.\n",
    "\n",
    "    Inputs:\n",
    "        x_data (pandas dataframe): data frame where each column is a predictor\n",
    "        y_data (pandas series): series of outcomes\n",
    "        threshold (float): probability threshold needed to call prediction positive\n",
    "        num_n (int): the number of neighbors\n",
    "    '''\n",
    "    mymodel = KNeighborsClassifier(n_neighbors=num_n)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('************KNN')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "def run_svm_model(x_data_scaled, y_data, x_test=None,\n",
    "                  y_test=None, kernel='linear', \n",
    "                  threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    Runs an SVM model on your data\n",
    "    Note: this will run much faster if you scale your x data first\n",
    "    '''\n",
    "    mymodel = svm.SVC(kernel='linear', probability=True)\n",
    "    mymodel.fit(x_data_scaled,\n",
    "                    y_data)  \n",
    "    mypreds = mymodel.predict(x_data_scaled)   \n",
    "    \n",
    "    print('************Random Forest')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data_scaled))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class']) \n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def run_forest(x_data, y_data, x_test=None, y_test=None,\n",
    "               my_n_estimators=10, my_max_depth=5,\n",
    "               threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    Runs a random forest model\n",
    "    Inputs:\n",
    "        x_data (pd dataframe) : our predictor data\n",
    "        y_data (pd dataframe) : our outcome data\n",
    "        n_estimators (int): the number of trees in our forest\n",
    "        max_depth (int): the max number of levels in our trees\n",
    "    '''\n",
    "    mymodel = RandomForestClassifier(n_estimators=my_n_estimators, max_depth=my_max_depth)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('************Random Forest')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "def run_boosted_model(x_data, y_data, x_test=None, y_test=None,\n",
    "                      my_max_depth=5, my_n_estimators=100,\n",
    "                      threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    Run a boosted decision tree model\n",
    "    '''\n",
    "    mymodel = AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=5),\n",
    "                             algorithm=\"SAMME\",\n",
    "                             n_estimators=10)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('************Boosted Decision Tree')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "\n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])    \n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "def run_bagging_model(x_data, y_data, x_test=None,\n",
    "                      y_test=None, threshold=50, use_test_sets=False):\n",
    "    '''\n",
    "    Runs a bagging model\n",
    "    '''    \n",
    "    mymodel = BaggingClassifier(KNeighborsClassifier(n_neighbors=16),\n",
    "                                 max_samples=100, max_features=1000)\n",
    "    mymodel.fit(x_data, y_data)\n",
    "    \n",
    "    print('************Bagged KNN')\n",
    "    print(\"*********Threshold:{0}\".format(threshold))\n",
    "    \n",
    "    if use_test_sets:\n",
    "        print('Returning test set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_test))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_test, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm\n",
    "    \n",
    "    else:\n",
    "        print('Returning training set performance:')\n",
    "        predicted_probs = pd.DataFrame(mymodel.predict_proba(x_data))\n",
    "        predicted_probs['predicted_class'] = 0\n",
    "        # get the cutoff to classify *threshold* percent of our rows as positive\n",
    "        cut_off = np.percentile(a=predicted_probs[1], q=threshold)\n",
    "        predicted_probs.loc[predicted_probs[1] >= cutoff,\n",
    "                            'predicted_class'] = 1\n",
    "        cm = metrics.confusion_matrix(y_data, predicted_probs['predicted_class'])\n",
    "        print_confusion_matrix(cm)\n",
    "        \n",
    "        return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looping through models to compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(x_data=None, x_data_scaled=None, y_data=None,\n",
    "                   x_test=None, x_test_scaled=None, y_test=None, use_test_data=False,\n",
    "                   run_bagging=False, run_boosted=False, run_a_forest=False,\n",
    "                   run_svm=False, run_knn=False, run_logit=False, run_tree=False,\n",
    "                   mythresholds=[50],\n",
    "                   my_max_depth=5, my_n_estimators=100,\n",
    "                   num_n=16,\n",
    "                   outcome_labels=None,\n",
    "                  mykernel='linear'):    \n",
    "    '''\n",
    "    Compare all our models\n",
    "    '''\n",
    "    model = []  # records the model we used\n",
    "    fpr = []    # records the false positive rate we got\n",
    "    tpr = []    # records the true positive rate we got\n",
    "    precision = []   # records the precision we got\n",
    "    current_threshold = []   # records teh threshold we used\n",
    "    \n",
    "    if run_bagging:\n",
    "        # in each of these if statements, we loop through the thresholds, recalculating the model\n",
    "        # each time.\n",
    "        for t in mythresholds:\n",
    "            cm = run_bagging_model(x_data, y_data, threshold=t, x_test=x_test,\n",
    "                                   y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))   # append the true positive rate result\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))   # append the false positive rate result\n",
    "            model.append('bagging')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))  # Append the precision result\n",
    "            print('\\n')\n",
    "    if run_boosted:\n",
    "        for t in mythresholds:\n",
    "            cm = run_boosted_model(x_data=x_data, y_data=y_data,\n",
    "                              my_max_depth=my_max_depth, my_n_estimators=my_n_estimators,\n",
    "                                   threshold=t, x_test=x_test, y_test=y_test,\n",
    "                                   use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('boosted')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_a_forest:\n",
    "        for t in mythresholds:\n",
    "            cm = run_forest(x_data, y_data, x_test, y_test,\n",
    "                        my_n_estimators, my_max_depth, t,\n",
    "                                use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('forest')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_svm:\n",
    "        for t in mythresholds:\n",
    "            cm = run_svm_model(x_data_scaled, y_data, threshold=t,\n",
    "                              x_test=x_test_scaled, y_test=y_test,\n",
    "                               use_test_sets=use_test_data, kernel=mykernel)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('svm')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_knn:\n",
    "        for t in mythresholds:\n",
    "            cm = run_knn_model(x_data, y_data, num_n=num_n, threshold=t,\n",
    "                              x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('knn')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_logit:\n",
    "        for t in mythresholds:\n",
    "            cm = run_logit_model(x_data, y_data=y_data, threshold=t,\n",
    "                                x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('logit')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "    if run_tree:\n",
    "        for t in mythresholds:\n",
    "            cm = run_tree_model(x_data,\n",
    "                       y_data=y_data,\n",
    "                       max_depth=my_max_depth,\n",
    "                       outcome_labels=outcome_labels, threshold=t,\n",
    "                               x_test=x_test, y_test=y_test, use_test_sets=use_test_data)\n",
    "            tpr.append(cm[1][1] / sum(cm[1]))\n",
    "            fpr.append(cm[0][1] / sum(cm[0]))\n",
    "            model.append('tree')\n",
    "            current_threshold.append(t)\n",
    "            precision.append(cm[1][1] / (cm[1][1] + cm[0][1]))\n",
    "            print('\\n')\n",
    "            \n",
    "            \n",
    "    # once our selected models have run, this piece of code creates a pandas dataframe with our\n",
    "    # different model results. We then export the table using a random number that identifies the\n",
    "    # model run, and plot a line plot of the performance (true positive rate over false positive rate)\n",
    "    # We also print the AUC for each model.\n",
    "    rows_to_add = len(current_threshold)\n",
    "    to_plot = pd.DataFrame({'model': model + ['baseline'] * rows_to_add, \n",
    "                            'tpr': tpr + fpr[0:rows_to_add],\n",
    "                            'fpr': fpr + fpr[0:rows_to_add],\n",
    "                            'precision': precision + [None] * rows_to_add,\n",
    "                            'threshold': current_threshold + [None] * rows_to_add,\n",
    "                           })\n",
    "    print('\\n\\n')\n",
    "    for m in to_plot['model'].unique():\n",
    "        auc = -1*np.trapz(y=to_plot.loc[to_plot['model'] == m, 'tpr'],\n",
    "                      x=to_plot.loc[to_plot['model'] == m, 'fpr'])\n",
    "        print('AUC for ', m, ' is', auc)\n",
    "        \n",
    "    print('\\n\\nTable of results:')\n",
    "    print(to_plot)\n",
    "    random_number = random.randint(1,100000)\n",
    "    print(random_number)\n",
    "    to_plot.to_csv(str(random_number) + \".csv\")\n",
    "    plt.clf()\n",
    "    sns.lineplot(x=to_plot['fpr'],\n",
    "                 y=to_plot['tpr'],\n",
    "                 hue=to_plot['model'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to split data into test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_using_date(data, train_start_date, train_end_date, test_start_date, test_end_date):\n",
    "    '''\n",
    "    Splits our data into test and training sets using a date the user provides\n",
    "    '''\n",
    "    train = projects.loc[projects['date_posted'].between(\n",
    "                                                    train_start_date,\n",
    "                                                    train_end_date,\n",
    "                                                    inclusive=True), :]\n",
    "    test = projects.loc[projects['date_posted'].between(\n",
    "                                                    test_start_date,\n",
    "                                                    test_end_date,\n",
    "                                                    inclusive=False), :]\n",
    "    \n",
    "    return [train, test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
